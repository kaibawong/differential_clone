{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Dated Brent  Bonny light  Mars   WTI  Maya Europe  Maya USGC  \\\n",
      "0    1/1/2000         25.5          0.0   0.0   0.0          0.0        0.0   \n",
      "1    1/2/2000         27.9          0.0   0.0   0.0          0.0        0.0   \n",
      "2    1/3/2000         27.3          0.0   0.0   0.0          0.0        0.0   \n",
      "3    1/4/2000         22.6          0.0   0.0   0.0          0.0        0.0   \n",
      "4    1/5/2000         27.6          0.0   0.0   0.0          0.0        0.0   \n",
      "..        ...          ...          ...   ...   ...          ...        ...   \n",
      "231  1/4/2019         71.3          1.4  -2.3  -7.4         -9.8       -6.5   \n",
      "232  1/5/2019         71.1          1.9  -4.3 -10.3        -10.1       -8.4   \n",
      "233  1/6/2019         64.1          1.7  -4.6  -9.4         -8.1       -5.2   \n",
      "234  1/7/2019         64.0          1.4  -2.0  -6.5         -6.9       -4.1   \n",
      "235  1/8/2019         59.0          1.3  -1.9  -4.2        -12.6       -8.0   \n",
      "\n",
      "     ESPO  Urals NWE  Urals MED  ...   FO 0.5%    VGO 2%  Arab light  \\\n",
      "0     0.0       -0.5       -0.5  ...  0.000000  0.276573        1.00   \n",
      "1     0.0       -0.5       -0.5  ...  0.000000  1.195334        0.75   \n",
      "2     0.0       -1.0       -1.5  ...  0.000000  1.038289        0.75   \n",
      "3     0.0       -1.0       -1.4  ...  0.000000 -0.616742        0.75   \n",
      "4     0.0       -0.6       -1.2  ...  0.000000 -1.653571        0.50   \n",
      "..    ...        ...        ...  ...       ...       ...         ...   \n",
      "231   1.7       -0.1        0.4  ... -3.102314  3.919293       -1.90   \n",
      "232   0.5       -0.9       -0.2  ... -5.629664  4.786823       -0.80   \n",
      "233  -0.2       -2.7       -2.5  ... -0.835000  5.262463        0.00   \n",
      "234   2.3       -1.1       -0.5  ...  4.105514  5.853681       -1.10   \n",
      "235   4.2       -0.1        1.2  ... -0.952231  7.948248       -2.70   \n",
      "\n",
      "     Arab Heavy  Azeri BTC  Tapis  Dalia  Oman  Oseberg blend    ORL  \n",
      "0           0.0        0.0    0.2    0.0  -1.3          0.000  0.000  \n",
      "1           0.0        0.0   -0.4    0.0  -2.5          0.000  0.000  \n",
      "2           0.0        0.0    1.2    0.0  -1.6          0.000  0.000  \n",
      "3           0.0        0.0    2.7    0.0   0.1          0.000  0.000  \n",
      "4           0.0        0.0    1.1    0.0  -1.8          0.000  0.000  \n",
      "..          ...        ...    ...    ...   ...            ...    ...  \n",
      "231        -4.4        1.8    3.4    0.6  -0.1          1.080 -5.900  \n",
      "232        -3.5        1.5    2.7    0.1  -1.3          1.522 -5.820  \n",
      "233        -3.0        0.2    2.1   -0.3  -2.3          1.617 -6.375  \n",
      "234        -4.0        1.6    3.8    1.2  -0.3          0.786 -6.990  \n",
      "235        -5.5        2.9    4.4    2.0   0.7          0.683 -6.928  \n",
      "\n",
      "[236 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "combi = pd.read_table('Historical_combi2.csv',delimiter =',')\n",
    "\n",
    "combi=combi.fillna(0)\n",
    "print(combi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#x_train=train_combi[[\"Dated Brent\",\"Jet\",\"ULSD\",\"VGO 2%\"]]\n",
    "\n",
    "X = combi[[\"Dated Brent\",\"Propane\",\"Gasoline\",\"Jet\",\"Gasoil\",\"FO 1%\",\"ULSD\",\"Naphtha\",\"Butane\",\"FO 3.5%\",\"VGO 0.5%\",\n",
    "           \"FO 0.5%\",\"VGO 2%\"]]\n",
    "y = combi[\"Urals NWE\"].values\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "sc= MinMaxScaler()\n",
    "X= sc.fit_transform(X)\n",
    "y= y.reshape(-1,1)\n",
    "y=sc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model and optimiser\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=13, input_dim=13))\n",
    "    regressor.add(Dense(10))\n",
    "    regressor.add(Dense(1))\n",
    "    regressor.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae','accuracy'])\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "regressor = KerasRegressor(build_fn=build_regressor, batch_size=32,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "165/165 [==============================] - 0s 908us/step - loss: 455.7265 - mae: 18.2701 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "165/165 [==============================] - 0s 97us/step - loss: 138.8212 - mae: 9.4740 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "165/165 [==============================] - 0s 215us/step - loss: 52.5976 - mae: 5.2726 - accuracy: 0.0303\n",
      "Epoch 4/200\n",
      "165/165 [==============================] - 0s 110us/step - loss: 64.0049 - mae: 5.2895 - accuracy: 0.0182\n",
      "Epoch 5/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 64.9624 - mae: 5.3325 - accuracy: 0.0121\n",
      "Epoch 6/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 46.9649 - mae: 4.4206 - accuracy: 0.0061\n",
      "Epoch 7/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 36.4679 - mae: 4.0476 - accuracy: 0.0242\n",
      "Epoch 8/200\n",
      "165/165 [==============================] - 0s 111us/step - loss: 34.2822 - mae: 4.3520 - accuracy: 0.0182\n",
      "Epoch 9/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 32.6508 - mae: 4.3388 - accuracy: 0.0182\n",
      "Epoch 10/200\n",
      "165/165 [==============================] - 0s 294us/step - loss: 28.9798 - mae: 3.8386 - accuracy: 0.0182\n",
      "Epoch 11/200\n",
      "165/165 [==============================] - 0s 100us/step - loss: 27.0335 - mae: 3.4976 - accuracy: 0.0242\n",
      "Epoch 12/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 25.6361 - mae: 3.4093 - accuracy: 0.0121\n",
      "Epoch 13/200\n",
      "165/165 [==============================] - 0s 359us/step - loss: 23.4700 - mae: 3.4129 - accuracy: 0.0182\n",
      "Epoch 14/200\n",
      "165/165 [==============================] - 0s 116us/step - loss: 22.3042 - mae: 3.4656 - accuracy: 0.0061\n",
      "Epoch 15/200\n",
      "165/165 [==============================] - 0s 346us/step - loss: 21.2585 - mae: 3.4022 - accuracy: 0.0121\n",
      "Epoch 16/200\n",
      "165/165 [==============================] - 0s 109us/step - loss: 20.1784 - mae: 3.2768 - accuracy: 0.0061\n",
      "Epoch 17/200\n",
      "165/165 [==============================] - 0s 134us/step - loss: 19.2472 - mae: 3.1402 - accuracy: 0.0121\n",
      "Epoch 18/200\n",
      "165/165 [==============================] - 0s 320us/step - loss: 18.4043 - mae: 3.0890 - accuracy: 0.0182\n",
      "Epoch 19/200\n",
      "165/165 [==============================] - 0s 113us/step - loss: 17.4470 - mae: 3.0658 - accuracy: 0.0182\n",
      "Epoch 20/200\n",
      "165/165 [==============================] - 0s 52us/step - loss: 16.7562 - mae: 2.9668 - accuracy: 0.0242\n",
      "Epoch 21/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 16.0368 - mae: 2.8654 - accuracy: 0.0121\n",
      "Epoch 22/200\n",
      "165/165 [==============================] - 0s 294us/step - loss: 15.1666 - mae: 2.9357 - accuracy: 0.0182\n",
      "Epoch 23/200\n",
      "165/165 [==============================] - 0s 111us/step - loss: 14.5545 - mae: 2.9997 - accuracy: 0.0303\n",
      "Epoch 24/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 13.7336 - mae: 2.8551 - accuracy: 0.0242\n",
      "Epoch 25/200\n",
      "165/165 [==============================] - 0s 300us/step - loss: 12.8332 - mae: 2.7666 - accuracy: 0.0303\n",
      "Epoch 26/200\n",
      "165/165 [==============================] - 0s 122us/step - loss: 12.2881 - mae: 2.7608 - accuracy: 0.0303\n",
      "Epoch 27/200\n",
      "165/165 [==============================] - 0s 118us/step - loss: 11.6338 - mae: 2.6571 - accuracy: 0.0364\n",
      "Epoch 28/200\n",
      "165/165 [==============================] - 0s 315us/step - loss: 11.0317 - mae: 2.5860 - accuracy: 0.0303\n",
      "Epoch 29/200\n",
      "165/165 [==============================] - 0s 116us/step - loss: 10.6002 - mae: 2.5428 - accuracy: 0.0364\n",
      "Epoch 30/200\n",
      "165/165 [==============================] - 0s 122us/step - loss: 9.9936 - mae: 2.4848 - accuracy: 0.0303\n",
      "Epoch 31/200\n",
      "165/165 [==============================] - 0s 324us/step - loss: 9.6472 - mae: 2.4646 - accuracy: 0.0303\n",
      "Epoch 32/200\n",
      "165/165 [==============================] - 0s 148us/step - loss: 9.0956 - mae: 2.3866 - accuracy: 0.0303\n",
      "Epoch 33/200\n",
      "165/165 [==============================] - 0s 124us/step - loss: 8.6518 - mae: 2.2767 - accuracy: 0.0242\n",
      "Epoch 34/200\n",
      "165/165 [==============================] - 0s 264us/step - loss: 8.3193 - mae: 2.2302 - accuracy: 0.0242\n",
      "Epoch 35/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 7.9084 - mae: 2.1938 - accuracy: 0.0242\n",
      "Epoch 36/200\n",
      "165/165 [==============================] - 0s 131us/step - loss: 7.6057 - mae: 2.1501 - accuracy: 0.0242\n",
      "Epoch 37/200\n",
      "165/165 [==============================] - 0s 374us/step - loss: 7.3048 - mae: 2.1316 - accuracy: 0.0303\n",
      "Epoch 38/200\n",
      "165/165 [==============================] - 0s 156us/step - loss: 6.9457 - mae: 2.0917 - accuracy: 0.0303\n",
      "Epoch 39/200\n",
      "165/165 [==============================] - 0s 349us/step - loss: 6.6389 - mae: 2.0416 - accuracy: 0.0242\n",
      "Epoch 40/200\n",
      "165/165 [==============================] - 0s 160us/step - loss: 6.3283 - mae: 1.9765 - accuracy: 0.0303\n",
      "Epoch 41/200\n",
      "165/165 [==============================] - 0s 138us/step - loss: 6.1606 - mae: 1.9199 - accuracy: 0.0303\n",
      "Epoch 42/200\n",
      "165/165 [==============================] - 0s 151us/step - loss: 5.8809 - mae: 1.8726 - accuracy: 0.0303\n",
      "Epoch 43/200\n",
      "165/165 [==============================] - 0s 139us/step - loss: 5.5893 - mae: 1.8518 - accuracy: 0.0303\n",
      "Epoch 44/200\n",
      "165/165 [==============================] - 0s 320us/step - loss: 5.4124 - mae: 1.8348 - accuracy: 0.0303\n",
      "Epoch 45/200\n",
      "165/165 [==============================] - 0s 142us/step - loss: 5.2228 - mae: 1.8042 - accuracy: 0.0303\n",
      "Epoch 46/200\n",
      "165/165 [==============================] - 0s 151us/step - loss: 4.9822 - mae: 1.7590 - accuracy: 0.0303\n",
      "Epoch 47/200\n",
      "165/165 [==============================] - 0s 345us/step - loss: 4.7838 - mae: 1.7277 - accuracy: 0.0303\n",
      "Epoch 48/200\n",
      "165/165 [==============================] - 0s 100us/step - loss: 4.6287 - mae: 1.7006 - accuracy: 0.0303\n",
      "Epoch 49/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 4.4942 - mae: 1.6614 - accuracy: 0.0364\n",
      "Epoch 50/200\n",
      "165/165 [==============================] - 0s 99us/step - loss: 4.3035 - mae: 1.6475 - accuracy: 0.0303\n",
      "Epoch 51/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 4.2417 - mae: 1.6489 - accuracy: 0.0303\n",
      "Epoch 52/200\n",
      "165/165 [==============================] - 0s 97us/step - loss: 4.0165 - mae: 1.5986 - accuracy: 0.0303\n",
      "Epoch 53/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 3.9091 - mae: 1.5422 - accuracy: 0.0424\n",
      "Epoch 54/200\n",
      "165/165 [==============================] - 0s 88us/step - loss: 3.8060 - mae: 1.5207 - accuracy: 0.0424\n",
      "Epoch 55/200\n",
      "165/165 [==============================] - 0s 255us/step - loss: 3.7305 - mae: 1.5442 - accuracy: 0.0364\n",
      "Epoch 56/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 3.5495 - mae: 1.4995 - accuracy: 0.0364\n",
      "Epoch 57/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 3.4358 - mae: 1.4560 - accuracy: 0.0424\n",
      "Epoch 58/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 3.3673 - mae: 1.4382 - accuracy: 0.0424\n",
      "Epoch 59/200\n",
      "165/165 [==============================] - 0s 249us/step - loss: 3.2253 - mae: 1.4123 - accuracy: 0.0424\n",
      "Epoch 60/200\n",
      "165/165 [==============================] - 0s 105us/step - loss: 3.1089 - mae: 1.3968 - accuracy: 0.0424\n",
      "Epoch 61/200\n",
      "165/165 [==============================] - 0s 129us/step - loss: 3.0227 - mae: 1.3847 - accuracy: 0.0364\n",
      "Epoch 62/200\n",
      "165/165 [==============================] - 0s 120us/step - loss: 2.9087 - mae: 1.3541 - accuracy: 0.0364\n",
      "Epoch 63/200\n",
      "165/165 [==============================] - 0s 100us/step - loss: 2.8659 - mae: 1.3436 - accuracy: 0.0364\n",
      "Epoch 64/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 2.7571 - mae: 1.3204 - accuracy: 0.0364\n",
      "Epoch 65/200\n",
      "165/165 [==============================] - 0s 269us/step - loss: 2.7129 - mae: 1.3116 - accuracy: 0.0364\n",
      "Epoch 66/200\n",
      "165/165 [==============================] - 0s 108us/step - loss: 2.6146 - mae: 1.2901 - accuracy: 0.0364\n",
      "Epoch 67/200\n",
      "165/165 [==============================] - 0s 112us/step - loss: 2.5743 - mae: 1.2701 - accuracy: 0.0424\n",
      "Epoch 68/200\n",
      "165/165 [==============================] - 0s 275us/step - loss: 2.5074 - mae: 1.2532 - accuracy: 0.0364\n",
      "Epoch 69/200\n",
      "165/165 [==============================] - 0s 107us/step - loss: 2.4256 - mae: 1.2392 - accuracy: 0.0364\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 0s 100us/step - loss: 2.3708 - mae: 1.2165 - accuracy: 0.0424\n",
      "Epoch 71/200\n",
      "165/165 [==============================] - 0s 114us/step - loss: 2.3104 - mae: 1.2014 - accuracy: 0.0364\n",
      "Epoch 72/200\n",
      "165/165 [==============================] - 0s 252us/step - loss: 2.2486 - mae: 1.1834 - accuracy: 0.0364\n",
      "Epoch 73/200\n",
      "165/165 [==============================] - 0s 106us/step - loss: 2.2035 - mae: 1.1724 - accuracy: 0.0364\n",
      "Epoch 74/200\n",
      "165/165 [==============================] - 0s 117us/step - loss: 2.1761 - mae: 1.1704 - accuracy: 0.0424\n",
      "Epoch 75/200\n",
      "165/165 [==============================] - 0s 281us/step - loss: 2.1076 - mae: 1.1554 - accuracy: 0.0364\n",
      "Epoch 76/200\n",
      "165/165 [==============================] - 0s 114us/step - loss: 2.0681 - mae: 1.1351 - accuracy: 0.0364\n",
      "Epoch 77/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 2.0150 - mae: 1.1226 - accuracy: 0.0364\n",
      "Epoch 78/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 1.9709 - mae: 1.1099 - accuracy: 0.0364\n",
      "Epoch 79/200\n",
      "165/165 [==============================] - 0s 115us/step - loss: 1.9560 - mae: 1.0980 - accuracy: 0.0364\n",
      "Epoch 80/200\n",
      "165/165 [==============================] - 0s 105us/step - loss: 1.9151 - mae: 1.0904 - accuracy: 0.0424\n",
      "Epoch 81/200\n",
      "165/165 [==============================] - 0s 102us/step - loss: 1.8585 - mae: 1.0843 - accuracy: 0.0424\n",
      "Epoch 82/200\n",
      "165/165 [==============================] - 0s 274us/step - loss: 1.8013 - mae: 1.0609 - accuracy: 0.0364\n",
      "Epoch 83/200\n",
      "165/165 [==============================] - 0s 107us/step - loss: 1.7868 - mae: 1.0522 - accuracy: 0.0364\n",
      "Epoch 84/200\n",
      "165/165 [==============================] - 0s 90us/step - loss: 1.7456 - mae: 1.0476 - accuracy: 0.0424\n",
      "Epoch 85/200\n",
      "165/165 [==============================] - 0s 272us/step - loss: 1.7147 - mae: 1.0374 - accuracy: 0.0424\n",
      "Epoch 86/200\n",
      "165/165 [==============================] - 0s 126us/step - loss: 1.6924 - mae: 1.0330 - accuracy: 0.0424\n",
      "Epoch 87/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 1.6856 - mae: 1.0325 - accuracy: 0.0424\n",
      "Epoch 88/200\n",
      "165/165 [==============================] - 0s 99us/step - loss: 1.6159 - mae: 0.9994 - accuracy: 0.0545\n",
      "Epoch 89/200\n",
      "165/165 [==============================] - 0s 249us/step - loss: 1.6305 - mae: 1.0029 - accuracy: 0.0485\n",
      "Epoch 90/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 1.5874 - mae: 0.9940 - accuracy: 0.0424\n",
      "Epoch 91/200\n",
      "165/165 [==============================] - 0s 100us/step - loss: 1.5430 - mae: 0.9844 - accuracy: 0.0424\n",
      "Epoch 92/200\n",
      "165/165 [==============================] - 0s 296us/step - loss: 1.5205 - mae: 0.9710 - accuracy: 0.0485\n",
      "Epoch 93/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 1.5532 - mae: 0.9798 - accuracy: 0.0485\n",
      "Epoch 94/200\n",
      "165/165 [==============================] - 0s 102us/step - loss: 1.4523 - mae: 0.9555 - accuracy: 0.0485\n",
      "Epoch 95/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 1.4679 - mae: 0.9630 - accuracy: 0.0424\n",
      "Epoch 96/200\n",
      "165/165 [==============================] - 0s 274us/step - loss: 1.4483 - mae: 0.9479 - accuracy: 0.0485\n",
      "Epoch 97/200\n",
      "165/165 [==============================] - 0s 91us/step - loss: 1.4171 - mae: 0.9383 - accuracy: 0.0485\n",
      "Epoch 98/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 1.3900 - mae: 0.9286 - accuracy: 0.0485\n",
      "Epoch 99/200\n",
      "165/165 [==============================] - 0s 305us/step - loss: 1.3842 - mae: 0.9236 - accuracy: 0.0545\n",
      "Epoch 100/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 1.3601 - mae: 0.9195 - accuracy: 0.0485\n",
      "Epoch 101/200\n",
      "165/165 [==============================] - 0s 105us/step - loss: 1.3966 - mae: 0.9382 - accuracy: 0.0424\n",
      "Epoch 102/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 1.3081 - mae: 0.9031 - accuracy: 0.0485\n",
      "Epoch 103/200\n",
      "165/165 [==============================] - 0s 113us/step - loss: 1.3365 - mae: 0.9010 - accuracy: 0.0485\n",
      "Epoch 104/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 1.2799 - mae: 0.8861 - accuracy: 0.0485\n",
      "Epoch 105/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 1.2876 - mae: 0.8977 - accuracy: 0.0485\n",
      "Epoch 106/200\n",
      "165/165 [==============================] - 0s 255us/step - loss: 1.2738 - mae: 0.8827 - accuracy: 0.0485\n",
      "Epoch 107/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 1.2402 - mae: 0.8722 - accuracy: 0.0485\n",
      "Epoch 108/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 1.2203 - mae: 0.8679 - accuracy: 0.0424\n",
      "Epoch 109/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 1.2160 - mae: 0.8665 - accuracy: 0.0424\n",
      "Epoch 110/200\n",
      "165/165 [==============================] - 0s 279us/step - loss: 1.1961 - mae: 0.8615 - accuracy: 0.0424\n",
      "Epoch 111/200\n",
      "165/165 [==============================] - 0s 93us/step - loss: 1.2067 - mae: 0.8516 - accuracy: 0.0545\n",
      "Epoch 112/200\n",
      "165/165 [==============================] - 0s 90us/step - loss: 1.2306 - mae: 0.8602 - accuracy: 0.0545\n",
      "Epoch 113/200\n",
      "165/165 [==============================] - 0s 94us/step - loss: 1.1963 - mae: 0.8600 - accuracy: 0.0485\n",
      "Epoch 114/200\n",
      "165/165 [==============================] - 0s 279us/step - loss: 1.1865 - mae: 0.8568 - accuracy: 0.0485\n",
      "Epoch 115/200\n",
      "165/165 [==============================] - 0s 103us/step - loss: 1.1411 - mae: 0.8370 - accuracy: 0.0485\n",
      "Epoch 116/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 1.1299 - mae: 0.8281 - accuracy: 0.0485\n",
      "Epoch 117/200\n",
      "165/165 [==============================] - 0s 270us/step - loss: 1.1190 - mae: 0.8276 - accuracy: 0.0485\n",
      "Epoch 118/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 1.1118 - mae: 0.8183 - accuracy: 0.0485\n",
      "Epoch 119/200\n",
      "165/165 [==============================] - 0s 90us/step - loss: 1.1069 - mae: 0.8128 - accuracy: 0.0485\n",
      "Epoch 120/200\n",
      "165/165 [==============================] - 0s 91us/step - loss: 1.0969 - mae: 0.8130 - accuracy: 0.0485\n",
      "Epoch 121/200\n",
      "165/165 [==============================] - 0s 268us/step - loss: 1.1263 - mae: 0.8285 - accuracy: 0.0424\n",
      "Epoch 122/200\n",
      "165/165 [==============================] - 0s 92us/step - loss: 1.0911 - mae: 0.8151 - accuracy: 0.0424\n",
      "Epoch 123/200\n",
      "165/165 [==============================] - 0s 92us/step - loss: 1.0601 - mae: 0.7932 - accuracy: 0.0545\n",
      "Epoch 124/200\n",
      "165/165 [==============================] - 0s 107us/step - loss: 1.0763 - mae: 0.7987 - accuracy: 0.0545\n",
      "Epoch 125/200\n",
      "165/165 [==============================] - 0s 270us/step - loss: 1.0521 - mae: 0.7996 - accuracy: 0.0424\n",
      "Epoch 126/200\n",
      "165/165 [==============================] - 0s 99us/step - loss: 1.0562 - mae: 0.7962 - accuracy: 0.0485\n",
      "Epoch 127/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 1.0680 - mae: 0.7969 - accuracy: 0.0606\n",
      "Epoch 128/200\n",
      "165/165 [==============================] - 0s 94us/step - loss: 1.0525 - mae: 0.7891 - accuracy: 0.0485\n",
      "Epoch 129/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 1.0581 - mae: 0.7945 - accuracy: 0.0424\n",
      "Epoch 130/200\n",
      "165/165 [==============================] - 0s 93us/step - loss: 1.0012 - mae: 0.7750 - accuracy: 0.0545\n",
      "Epoch 131/200\n",
      "165/165 [==============================] - 0s 91us/step - loss: 0.9915 - mae: 0.7681 - accuracy: 0.0485\n",
      "Epoch 132/200\n",
      "165/165 [==============================] - 0s 287us/step - loss: 1.0050 - mae: 0.7721 - accuracy: 0.0545\n",
      "Epoch 133/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 0.9781 - mae: 0.7622 - accuracy: 0.0545\n",
      "Epoch 134/200\n",
      "165/165 [==============================] - 0s 94us/step - loss: 0.9707 - mae: 0.7580 - accuracy: 0.0485\n",
      "Epoch 135/200\n",
      "165/165 [==============================] - 0s 140us/step - loss: 0.9991 - mae: 0.7697 - accuracy: 0.0667\n",
      "Epoch 136/200\n",
      "165/165 [==============================] - 0s 233us/step - loss: 0.9590 - mae: 0.7520 - accuracy: 0.0545\n",
      "Epoch 137/200\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.9709 - mae: 0.7575 - accuracy: 0.0545\n",
      "Epoch 138/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 0.9523 - mae: 0.7502 - accuracy: 0.0485\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 0s 313us/step - loss: 0.9378 - mae: 0.7440 - accuracy: 0.0485\n",
      "Epoch 140/200\n",
      "165/165 [==============================] - 0s 73us/step - loss: 0.9381 - mae: 0.7434 - accuracy: 0.0545\n",
      "Epoch 141/200\n",
      "165/165 [==============================] - 0s 64us/step - loss: 0.9265 - mae: 0.7378 - accuracy: 0.0545\n",
      "Epoch 142/200\n",
      "165/165 [==============================] - 0s 86us/step - loss: 0.9208 - mae: 0.7337 - accuracy: 0.0485\n",
      "Epoch 143/200\n",
      "165/165 [==============================] - 0s 278us/step - loss: 0.9202 - mae: 0.7316 - accuracy: 0.0606\n",
      "Epoch 144/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 0.9097 - mae: 0.7303 - accuracy: 0.0667\n",
      "Epoch 145/200\n",
      "165/165 [==============================] - 0s 100us/step - loss: 0.9232 - mae: 0.7317 - accuracy: 0.0485\n",
      "Epoch 146/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 0.8992 - mae: 0.7273 - accuracy: 0.0606\n",
      "Epoch 147/200\n",
      "165/165 [==============================] - 0s 275us/step - loss: 0.9073 - mae: 0.7289 - accuracy: 0.0667\n",
      "Epoch 148/200\n",
      "165/165 [==============================] - 0s 110us/step - loss: 0.9082 - mae: 0.7287 - accuracy: 0.0667\n",
      "Epoch 149/200\n",
      "165/165 [==============================] - 0s 107us/step - loss: 0.8998 - mae: 0.7213 - accuracy: 0.0667\n",
      "Epoch 150/200\n",
      "165/165 [==============================] - 0s 272us/step - loss: 0.8868 - mae: 0.7159 - accuracy: 0.0545\n",
      "Epoch 151/200\n",
      "165/165 [==============================] - 0s 106us/step - loss: 0.8848 - mae: 0.7151 - accuracy: 0.0485\n",
      "Epoch 152/200\n",
      "165/165 [==============================] - 0s 95us/step - loss: 0.8820 - mae: 0.7137 - accuracy: 0.0485\n",
      "Epoch 153/200\n",
      "165/165 [==============================] - 0s 96us/step - loss: 0.8725 - mae: 0.7104 - accuracy: 0.0545\n",
      "Epoch 154/200\n",
      "165/165 [==============================] - 0s 244us/step - loss: 0.9043 - mae: 0.7243 - accuracy: 0.0667\n",
      "Epoch 155/200\n",
      "165/165 [==============================] - 0s 110us/step - loss: 0.8775 - mae: 0.7171 - accuracy: 0.0667\n",
      "Epoch 156/200\n",
      "165/165 [==============================] - 0s 100us/step - loss: 0.8992 - mae: 0.7147 - accuracy: 0.0606\n",
      "Epoch 157/200\n",
      "165/165 [==============================] - 0s 267us/step - loss: 0.8966 - mae: 0.7102 - accuracy: 0.0667\n",
      "Epoch 158/200\n",
      "165/165 [==============================] - 0s 110us/step - loss: 0.9114 - mae: 0.7290 - accuracy: 0.0667\n",
      "Epoch 159/200\n",
      "165/165 [==============================] - 0s 113us/step - loss: 0.8599 - mae: 0.6998 - accuracy: 0.0606\n",
      "Epoch 160/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 0.8525 - mae: 0.7015 - accuracy: 0.0545\n",
      "Epoch 161/200\n",
      "165/165 [==============================] - 0s 258us/step - loss: 0.8382 - mae: 0.6939 - accuracy: 0.0667\n",
      "Epoch 162/200\n",
      "165/165 [==============================] - 0s 114us/step - loss: 0.8400 - mae: 0.6952 - accuracy: 0.0667\n",
      "Epoch 163/200\n",
      "165/165 [==============================] - 0s 139us/step - loss: 0.8417 - mae: 0.6887 - accuracy: 0.0727\n",
      "Epoch 164/200\n",
      "165/165 [==============================] - 0s 280us/step - loss: 0.8465 - mae: 0.6993 - accuracy: 0.0727\n",
      "Epoch 165/200\n",
      "165/165 [==============================] - 0s 150us/step - loss: 0.8265 - mae: 0.6872 - accuracy: 0.0667\n",
      "Epoch 166/200\n",
      "165/165 [==============================] - 0s 128us/step - loss: 0.8213 - mae: 0.6849 - accuracy: 0.0667\n",
      "Epoch 167/200\n",
      "165/165 [==============================] - 1s 6ms/step - loss: 0.8249 - mae: 0.6877 - accuracy: 0.0667\n",
      "Epoch 168/200\n",
      "165/165 [==============================] - 0s 73us/step - loss: 0.8358 - mae: 0.6883 - accuracy: 0.0727\n",
      "Epoch 169/200\n",
      "165/165 [==============================] - 0s 94us/step - loss: 0.8350 - mae: 0.6911 - accuracy: 0.0667\n",
      "Epoch 170/200\n",
      "165/165 [==============================] - 0s 101us/step - loss: 0.8103 - mae: 0.6784 - accuracy: 0.0667\n",
      "Epoch 171/200\n",
      "165/165 [==============================] - 0s 97us/step - loss: 0.8217 - mae: 0.6845 - accuracy: 0.0727\n",
      "Epoch 172/200\n",
      "165/165 [==============================] - 0s 94us/step - loss: 0.8247 - mae: 0.6847 - accuracy: 0.0667\n",
      "Epoch 173/200\n",
      "165/165 [==============================] - 0s 247us/step - loss: 0.8054 - mae: 0.6771 - accuracy: 0.0727\n",
      "Epoch 174/200\n",
      "165/165 [==============================] - 0s 113us/step - loss: 0.8072 - mae: 0.6791 - accuracy: 0.0667\n",
      "Epoch 175/200\n",
      "165/165 [==============================] - 0s 123us/step - loss: 0.7963 - mae: 0.6680 - accuracy: 0.0727\n",
      "Epoch 176/200\n",
      "165/165 [==============================] - 0s 286us/step - loss: 0.8129 - mae: 0.6768 - accuracy: 0.0788\n",
      "Epoch 177/200\n",
      "165/165 [==============================] - 0s 126us/step - loss: 0.7951 - mae: 0.6664 - accuracy: 0.0727\n",
      "Epoch 178/200\n",
      "165/165 [==============================] - 0s 102us/step - loss: 0.8198 - mae: 0.6739 - accuracy: 0.0667\n",
      "Epoch 179/200\n",
      "165/165 [==============================] - 0s 292us/step - loss: 0.7912 - mae: 0.6676 - accuracy: 0.0667\n",
      "Epoch 180/200\n",
      "165/165 [==============================] - 0s 112us/step - loss: 0.8474 - mae: 0.6833 - accuracy: 0.0727\n",
      "Epoch 181/200\n",
      "165/165 [==============================] - 0s 113us/step - loss: 0.8110 - mae: 0.6729 - accuracy: 0.0727\n",
      "Epoch 182/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 0.7932 - mae: 0.6731 - accuracy: 0.0727\n",
      "Epoch 183/200\n",
      "165/165 [==============================] - 0s 289us/step - loss: 0.7751 - mae: 0.6602 - accuracy: 0.0788\n",
      "Epoch 184/200\n",
      "165/165 [==============================] - 0s 107us/step - loss: 0.7778 - mae: 0.6550 - accuracy: 0.0727\n",
      "Epoch 185/200\n",
      "165/165 [==============================] - 0s 90us/step - loss: 0.7733 - mae: 0.6577 - accuracy: 0.0788\n",
      "Epoch 186/200\n",
      "165/165 [==============================] - 0s 297us/step - loss: 0.7801 - mae: 0.6482 - accuracy: 0.0788\n",
      "Epoch 187/200\n",
      "165/165 [==============================] - 0s 107us/step - loss: 0.7659 - mae: 0.6582 - accuracy: 0.0788\n",
      "Epoch 188/200\n",
      "165/165 [==============================] - 0s 98us/step - loss: 0.8017 - mae: 0.6749 - accuracy: 0.0727\n",
      "Epoch 189/200\n",
      "165/165 [==============================] - 0s 94us/step - loss: 0.7699 - mae: 0.6496 - accuracy: 0.0727\n",
      "Epoch 190/200\n",
      "165/165 [==============================] - 0s 249us/step - loss: 0.7510 - mae: 0.6484 - accuracy: 0.0727\n",
      "Epoch 191/200\n",
      "165/165 [==============================] - 0s 115us/step - loss: 0.7774 - mae: 0.6681 - accuracy: 0.0727\n",
      "Epoch 192/200\n",
      "165/165 [==============================] - 0s 109us/step - loss: 0.8097 - mae: 0.6650 - accuracy: 0.0788\n",
      "Epoch 193/200\n",
      "165/165 [==============================] - 0s 297us/step - loss: 0.7711 - mae: 0.6643 - accuracy: 0.0727\n",
      "Epoch 194/200\n",
      "165/165 [==============================] - 0s 105us/step - loss: 0.8510 - mae: 0.7058 - accuracy: 0.0727\n",
      "Epoch 195/200\n",
      "165/165 [==============================] - 0s 104us/step - loss: 0.7782 - mae: 0.6581 - accuracy: 0.0788\n",
      "Epoch 196/200\n",
      "165/165 [==============================] - 0s 99us/step - loss: 0.7445 - mae: 0.6404 - accuracy: 0.0788\n",
      "Epoch 197/200\n",
      "165/165 [==============================] - 0s 117us/step - loss: 0.7502 - mae: 0.6533 - accuracy: 0.0788\n",
      "Epoch 198/200\n",
      "165/165 [==============================] - 0s 112us/step - loss: 0.7340 - mae: 0.6395 - accuracy: 0.0788\n",
      "Epoch 199/200\n",
      "165/165 [==============================] - 0s 112us/step - loss: 0.7410 - mae: 0.6383 - accuracy: 0.0788\n",
      "Epoch 200/200\n",
      "165/165 [==============================] - 0s 251us/step - loss: 0.7377 - mae: 0.6381 - accuracy: 0.0788\n"
     ]
    }
   ],
   "source": [
    "results=regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2769943 ,  0.47868878, -0.630202  , -0.6220825 , -0.01420651,\n",
       "       -0.29290828, -0.79755664, -0.04937331], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input = pd.read_table('cracks_2019_input.csv',delimiter =',')\n",
    "y_pred= regressor.predict(X_input)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228    0.0\n",
       "229   -0.4\n",
       "230   -0.3\n",
       "231   -0.1\n",
       "232   -0.9\n",
       "233   -2.7\n",
       "234   -1.1\n",
       "235   -0.1\n",
       "Name: Urals NWE, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy= combi[\"Urals NWE\"].iloc[-8:]\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, 0.5)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(yy,y_pred)\n",
    "plt.xlim([-2, 0.5])\n",
    "plt.ylim([-2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-f962d051b443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Measured'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4442\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4444\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_pred)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
